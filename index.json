[{"authors":["sanketkshah"],"categories":null,"content":"I am a fourth-year PhD student at Harvard University advised by Prof. Milind Tambe. My current work focuses on Decision-Focused Learning, a paradigm for tailoring a predictive model for a downstream optimization task that uses its predictions.\nPreviously, I was Research Engineer at Singapore Management University (SMU) advised by Prof. Pradeep Varakantham where I used Reinforcement Learning to solve problems in Transportation and Security. I also spent a year at Microsoft Research India during which I worked on Information and Communications Technology for Development (ICTD) with Dr. Colin Scott and Dr. Bill Thies and Natural Language Processing with Dr. Sundararajan Sellamanickam.\n","date":1576368000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1576368000,"objectID":"95bcc4364e28c7ca3e505122179a5f57","permalink":"https://sanketkshah.github.io/authors/sanketkshah/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/sanketkshah/","section":"authors","summary":"I am a fourth-year PhD student at Harvard University advised by Prof. Milind Tambe. My current work focuses on Decision-Focused Learning, a paradigm for tailoring a predictive model for a downstream optimization task that uses its predictions.\nPreviously, I was Research Engineer at Singapore Management University (SMU) advised by Prof. Pradeep Varakantham where I used Reinforcement Learning to solve problems in Transportation and Security. I also spent a year at Microsoft Research India during which I worked on Information and Communications Technology for Development (ICTD) with Dr.","tags":null,"title":"Sanket Shah","type":"authors"},{"authors":["Sanket Shah","Arun Suggala","Milind Tambe","Aparna Taneja"],"categories":null,"content":"","date":1683331200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1683331200,"objectID":"19a85f647cfd033e0ad4738f423ca8c8","permalink":"https://sanketkshah.github.io/publication/dfl-rmabs-exact/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/publication/dfl-rmabs-exact/","section":"publication","summary":"We propose an efficient way to implement decision-focused learning for the kinds of RMABs used for intervention planning in public health.","tags":["Decision-Focused Learning","Machine Learning"],"title":"Efficient Public Health Intervention Planning Using Decomposition-Based Decision-Focused Learning","type":"publication"},{"authors":["Sanket Shah","Bryan Wilder","Andrew Perrault","Milind Tambe"],"categories":null,"content":"","date":1677196800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677196800,"objectID":"3abba7ff0c44163a7f6c4cce465d911e","permalink":"https://sanketkshah.github.io/publication/dfl-egl/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/publication/dfl-egl/","section":"publication","summary":"We propose two new innovations to help improve the learning of task-specific loss functions.","tags":["Decision-Focused Learning","Machine Learning"],"title":"Leaving the Nest: Going Beyond Local Loss Functions for Predict-Then-Optimize","type":"publication"},{"authors":["Kai Wang","Shresth Verma","Aditya Mate","Sanket Shah","Aparna Taneja","Neha Madhiwalla","Aparna Hegde","Milind Tambe"],"categories":null,"content":"","date":1675728000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675728000,"objectID":"295ea8340d966801595a0aca382a473c","permalink":"https://sanketkshah.github.io/publication/dfl-armann/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/publication/dfl-armann/","section":"publication","summary":"We propose a way to differentiate through MDP Planning for Restless Multi-Armed Bandits. We use this approach to better learn the Transition Matrices from \"features\" associated with different arms using Decision-Focused Learning.","tags":["Decision-Focused Learning","Reinforcement Learning","Restless Multi-Armed Bandits"],"title":"Decision-Focused Learning in Restless Multi-Armed Bandits with Application to Maternal and Child Care Domain","type":"publication"},{"authors":["Sanket Shah","Kai Wang","Bryan Wilder","Andrew Perrault","Milind Tambe"],"categories":null,"content":"","date":1671062400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1671062400,"objectID":"1cf4ade7bad5ee96ab3afa31884f743a","permalink":"https://sanketkshah.github.io/publication/dfl-lodl/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/publication/dfl-lodl/","section":"publication","summary":"We learn task-specific loss functions that, when trained on, allow a predictive model to make better predictions for the given task.","tags":["Decision-Focused Learning","Machine Learning"],"title":"Decision-Focused Learning without Decision-Making: Learning Locally Optimized Decision Losses","type":"publication"},{"authors":["Sanket Shah","Meghna Lowalekar","Pradeep Varakantham"],"categories":null,"content":"","date":1655078400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655078400,"objectID":"6cb6da4830c8b56037756d4e4458902a","permalink":"https://sanketkshah.github.io/publication/pricing-matching/","publishdate":"2022-06-13T00:00:00Z","relpermalink":"/publication/pricing-matching/","section":"publication","summary":"We learn how to price in ride-pooling (UberPool) while taking into account the matchings the pricing system induces.","tags":["Ride-Pooling","Transportation","Mechanism Design"],"title":"Joint Pricing and Matching for City-Scale Ride-Pooling","type":"publication"},{"authors":["Kai Wang","Sanket Shah","Haipeng Chen","Andrew Perrault","Finale Doshi-Velez","Milind Tambe"],"categories":null,"content":"","date":1638748800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638748800,"objectID":"a0c154815df8fe56a46eb1e1e9375b91","permalink":"https://sanketkshah.github.io/publication/dfl-mdp/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/publication/dfl-mdp/","section":"publication","summary":"We propose a way to optimally differentiate through Reinforcement Learning. Specifically, we propose two optimality conditions that hold at convergence and show how to (approximately) calculate gradients using them.","tags":["Decision-Focused Learning","Reinforcement Learning","Implicit Layers"],"title":"Learning MDPs from Features: Predict-Then-Optimize for Sequential Decision Problems by Reinforcement Learning","type":"publication"},{"authors":["Ashwin Kumar","Sanket Shah","Meghna Lowalekar","Pradeep Varakantham","Alvitta Ottley","William Yeoh"],"categories":null,"content":"","date":1629504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629504000,"objectID":"93f7d09d6c16f5bdea16e27ef47e3ff8","permalink":"https://sanketkshah.github.io/publication/fairvizard/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/publication/fairvizard/","section":"publication","summary":"We create a visualization to allow various stakeholders to evaluate the tradeoffs associated with multi-party fairness in the Ride-Pooling (Uber) ecosystem.","tags":["Ride-Pooling","Transportation","Fairness","Visualization"],"title":"FairVizARD: A Visualization System for Assessing Fairness of Ride-Sharing Matching Algorithms","type":"publication"},{"authors":["Naveen Raman","Sanket Shah","John Dickerson"],"categories":null,"content":"","date":1627776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627776000,"objectID":"abbef50b73dcaba333c5a7f83ad5fbf4","permalink":"https://sanketkshah.github.io/publication/fairness-naveen/","publishdate":"2021-08-01T00:00:00Z","relpermalink":"/publication/fairness-naveen/","section":"publication","summary":"We evaluate the efficacy of two different strategies of enforcing fairness in ride-pooling.","tags":["Ride-Pooling","Transportation","Fairness"],"title":"Data-Driven Methods for Balancing Fairness and Efficiency in Ride-Pooling","type":"publication"},{"authors":["Jackson Killian","Arpita Biswas","Sanket Shah","Milind Tambe"],"categories":null,"content":"","date":1597363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597363200,"objectID":"94393ac0e934a0cd2c3f7a0c78c0069b","permalink":"https://sanketkshah.github.io/publication/multi-action-rmab/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/publication/multi-action-rmab/","section":"publication","summary":"We propose two online model-free algorithms to learn the Whittle Index associated with *multi-action* Restless Multi-Armed Bandits.","tags":["Reinforcement Learning","Restless Multi-Armed Bandits"],"title":"Q-Learning Lagrange Policies for Multi-Action Restless Bandits","type":"publication"},{"authors":["Sanket Shah","Meghna Lowalekar","Pradeep Varakantham"],"categories":null,"content":"","date":1581033600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581033600,"objectID":"fd9f7ebb9c9b8fa0399125c97cb21786","permalink":"https://sanketkshah.github.io/publication/ride-pooling/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/publication/ride-pooling/","section":"publication","summary":"We add future information to ride-pooling assignments by using a novel extension to Approximate Dynamic Programming.","tags":["Ride-Pooling","Transportation","Approximate Dynamic Programming","Deep Reinforcement Learning"],"title":"Neural Approximate Dynamic Programming for On-Demand Ride-Pooling","type":"publication"},{"authors":["Sanket Shah","Arunesh Sinha","Pradeep Varakantham","Andrew Perrault","Milind Tambe"],"categories":null,"content":"","date":1581033600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581033600,"objectID":"b068e258bba8a1ebfd53d4e2b54a8319","permalink":"https://sanketkshah.github.io/publication/tsg/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/publication/tsg/","section":"publication","summary":"We reformulate Threat Screening Games, a kind of Stackelberg Security Game, as an MDP with constraints on the action space.","tags":["Game Theory","Deep Reinforcement Learning","Security"],"title":"Solving Online Threat Screening Games using Constrained Action Space Reinforcement Learning","type":"publication"},{"authors":["Sanket Shah"],"categories":["Blog"],"content":"Before joining SMU, I lived in my hometown of Bangalore in India. It is a city that, much to my chagrin, has become synonymous with its terrible traffic. The absence of comprehensive public transport has meant that, often, the most convenient alternative to owning a vehicle is in using taxi-on-demand services from aggregators like Uber and Ola. The affordability of ride-pooling services (like UberPool) have made them especially integral to the transportation ecosystem with taxi-sharing often being the cheapest means of on-demand transportation. However, while ride-pooling services are popular, they are far from efficient. I would often find myself trapped in circuitous two-hour long UberPool journeys, obsessing over ways to do things better. As a result, when an opportunity to work with my current advisor on such ride-pooling systems presented itself, I jumped at it.\nAt SMU, I started thinking of solutions to this problem in earnest\u0026mdash;how do you match customers to vehicles so that people would no longer have to circumnavigate Bangalore on their way to work. With lots of help from my collaborators, I went through the literature, especially the erstwhile state-of-the-art method for high-capacity city-scale ride-pooling \u0026lsquo;On-demand high-capacity ride-sharing via dynamic trip-vehicle assignment\u0026rsquo;. In this approach, they enumerated the feasible assignments of passengers to a vehicle and then ran an optimisation over these feasible assignments to choose the best combination of assignments over all vehicles. The time-consuming portion of this process was in enumerating these feasible assignments because there are an exponential number of them (in the worst case) and each involves solving an NP-hard routing problem. As a result, in practice, one can only generate a subset of all the feasible assignments.\nAccordingly, given that all feasible assignments can\u0026rsquo;t be enumerated, our first attempt at improving their solution was in trying to generate a \u0026lsquo;better\u0026rsquo; subset of them. We did this by \u0026rsquo;learning to search\u0026rsquo;, which is similar to A* search in which the heuristic is learned. The novel idea here was to make learning this heuristic unsupervised by using the weak supervision from the output of the optimisation problem\u0026mdash;we scored assignments based on how likely they were to be chosen as the optimal assignment. We found, however, that enumerating \u0026lsquo;better\u0026rsquo; (or even more) assignments didn\u0026rsquo;t improve the solution quality much. Instead, from visualising some trajectories, we found that a bigger issue was that the optimisation was greedy, i.e., it tried to maximise the matches at the current time-step without thought for how this would affect future matches.\nTo solve this problem, during the optimisation step, we tried to score the enumerated feasible assignments based on their expected future value rather than their immediate value. This idea has connections to learning a value function in the Q-Learning approach to RL, in which the optimal policy involves choosing the \u0026lsquo;action\u0026rsquo; (overall assignment) with the maximum future value. The key insight was that, if we could find a way to estimate this future value for a given feasible assignment, we could use the existing optimisation over feasible assignments to approximate this \u0026lsquo;max\u0026rsquo; operation in Q-Learning. Along these lines, I developed an algorithm for estimating this future value of a feasible assignment and tested it on real-world data \u0026ndash; it worked! We found that it outperformed the greedy method by up to 16% (which is huge on a city-scale).\nThis is where I thought the project would end. However, we hit a surprising roadblock \u0026ndash; we learned that, although the algorithm was \u0026lsquo;inspired\u0026rsquo; by Q-Learning, the modifications we made to it meant that it was no longer straightforward to use the theoretical results associated with Q-Learning to illustrate the soundness of our algorithm. The final part of this project involved finding a more \u0026rsquo;natural\u0026rsquo; way to communicate what the algorithm was doing and why it worked. To this end, we did some reading and found that we had re-invented Approximate Dynamic Programming (ADP), a framework from Operations Research that could be seen as a way to combine traditional optimisation with learned value functions to perform stochastic planning. The discovery allowed us to use the language and theory that they had (much more rigorously) developed to better analyse and discuss our algorithm.\nIf you are interested in learning more about the technical details of our algorithm, please check out the resulting paper here. The paper was recently accepted for Oral presentation at AAAI-20\u0026rsquo;s special track on AI for Social Impact. If you are planning to attend the conference, please drop by for our presentation! We are also currently looking for partners with whom to run a pilot study. If that\u0026rsquo;s you, please reach out.\n","date":1576368000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576368000,"objectID":"09f992d7e915d2882a74e1c8699960d4","permalink":"https://sanketkshah.github.io/post/ride-pooling/","publishdate":"2019-12-15T00:00:00Z","relpermalink":"/post/ride-pooling/","section":"post","summary":"A blog post about the joyride my paper 'Neural Approximate Dynamic Programming for On-Demand Ride-Pooling' took me on.","tags":["Ride-Pooling","Transportation","Approximate Dynamic Programming","Deep Reinforcement Learning"],"title":"My Ride-pooling Journey","type":"post"}]