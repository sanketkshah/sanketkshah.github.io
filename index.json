[{"authors":["sanketkshah"],"categories":null,"content":"\u0026#x26a0;\u0026#xfe0f;I am looking for postdocs!\u0026#x26a0;\u0026#xfe0f;\nI am a fifth-year PhD student at Harvard University advised by Prof. Milind Tambe. My current work focuses on Decision-Focused Learning, a way to improve the performance of machine learning models in algorithmic decision making by tailoring them to their downstream decision-making use cases.\nPreviously, I was Research Engineer at Singapore Management University (SMU) advised by Prof. Pradeep Varakantham where I used Reinforcement Learning to solve problems in Transportation and Security. I also spent a year at Microsoft Research India during which I worked on Information and Communications Technology for Development (ICTD) with Dr. Colin Scott and Dr. Bill Thies and Natural Language Processing with Dr. Sundararajan Sellamanickam.\n","date":1714521600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1714521600,"objectID":"95bcc4364e28c7ca3e505122179a5f57","permalink":"https://sanketkshah.github.io/authors/sanketkshah/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/sanketkshah/","section":"authors","summary":"\u0026#x26a0;\u0026#xfe0f;I am looking for postdocs!\u0026#x26a0;\u0026#xfe0f;\nI am a fifth-year PhD student at Harvard University advised by Prof. Milind Tambe. My current work focuses on Decision-Focused Learning, a way to improve the performance of machine learning models in algorithmic decision making by tailoring them to their downstream decision-making use cases.\nPreviously, I was Research Engineer at Singapore Management University (SMU) advised by Prof. Pradeep Varakantham where I used Reinforcement Learning to solve problems in Transportation and Security.","tags":null,"title":"Sanket Shah","type":"authors"},{"authors":["Niclas Boehmer","Yash Nair","Sanket Shah","Lucas Janson","Aparna Taneja","Milind Tambe"],"categories":null,"content":"","date":1724544000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1724544000,"objectID":"e25a862caa0a568129a9a9e0118ad0dd","permalink":"https://sanketkshah.github.io/publication/rct-evaluation/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/publication/rct-evaluation/","section":"publication","summary":"We show how a connection to the 'individualized treatment rules' literature in statistics to create asymptotically valid confidence intervals for RCTs that measure the quality of 'clever' intervention policies.","tags":["Machine Learning","Statistics","Causal Inference"],"title":"Evaluating the Effectiveness of Index-Based Treatment Allocation","type":"publication"},{"authors":["Shresth Verma","Yunfan Zhao","Sanket Shah","Niclas Boehmer","Aparna Taneja","Milind Tambe"],"categories":null,"content":"","date":1720224000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720224000,"objectID":"e1ee41bdfd66571a897a8feb498d7956","permalink":"https://sanketkshah.github.io/publication/dfl-fairness/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/publication/dfl-fairness/","section":"publication","summary":"We show how decision-focused learning can improve the fairness-accuracy tradeoff in algorithmic decision-making using a case study on a real-world domain.","tags":["Decision-Focused Learning","Machine Learning","Fairness"],"title":"Group Fairness in Predict-Then-Optimize Settings for Restless Bandits","type":"publication"},{"authors":["Sanket Shah","Arun Suggala","Milind Tambe","Aparna Taneja"],"categories":null,"content":"","date":1714953600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1714953600,"objectID":"19a85f647cfd033e0ad4738f423ca8c8","permalink":"https://sanketkshah.github.io/publication/dfl-rmabs-exact/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/publication/dfl-rmabs-exact/","section":"publication","summary":"We propose an efficient way to implement decision-focused learning for the kinds of RMABs used for intervention planning in public health.","tags":["Decision-Focused Learning","Machine Learning"],"title":"Efficient Public Health Intervention Planning Using Decomposition-Based Decision-Focused Learning","type":"publication"},{"authors":["Sanket Shah"],"categories":["Blog"],"content":" Introduction PtO is a framework for using machine learning to perform decision-making under uncertainty. As the name suggests, it proceeds in two steps—first, you make predictions about the uncertain quantities of interest and then, second, you make the required decisions assuming that these predictions are accurate. However, these decisions are only optimal if the input predictions are accurate. To evaluate the quality of our decisions for a given prediction, we check how well they would perform on the ground truth values of the quantities of interest (from the dataset) as opposed to the predictions. Let’s use an example to make all these different steps in the PtO pipeline concrete.\nAn Example Imagine that you are an advertiser and you want to decide which websites to advertise on. One way in which you could use ML to solve this problem would be as follows.\nPredict Step: First, you use some features (e.g., website metadata) to predict the probability of a certain type of user clicking on your ad if it was advertised on a given website. You repeat this for all user types (e.g., demographic groups) and websites, resulting in the creation of a table like the one below, in which each row corresponds to a choice of website, and each column corresponds to a user type. Optimize Step: Given these predictions, you use them to parameterize an optimization problem that assumes that these predictions are accurate and creates a decision that would maximize your desired objective. Perhaps, the goal is to choose the subset of websites to advertise on in order to maximize the number of user types who click on the ad at least once. This decision can be made by solving the submodular maximization problem below, in which ŷ are predictions from the table and z is the probability of choosing a given website. Evaluate Step: Finally, you evaluate the quality of your produced decision, say the decision to advertise on websites 1 and 3 (and not 2). To do this, you calculate the expected number of user types that would have clicked on the ad at least once using the historical data\u0026rsquo;s ground truth probabilities (not the predictions!). We call this value of 0.89 below the “decision quality” of our predictions. Technical Challenge The example above describes one way in which ML, and this PtO framework specifically, can be used to perform decision-making under uncertainty, but what is the technical challenge here? The naive way in which to use ML for PtO would be to first train some predictive model to maximize predictive accuracy, and then, later deploy it to make decisions. However, as you can see in the figure below, this is suboptimal because what the model was trained for and what it’s being used for are different things.\nConsequently, the big question in this PtO setting is about how to learn predictive models that maximize the downstream “decision quality” for the downstream task of interest. We call this paradigm “Decision-Focused Learning”, and in practice, we find that it is particularly relevant for the kinds of small data + small model settings that are prevalent in AI for Social Impact.\nPast Work Conceptually, the easiest way to learn decision-focused predictive models is to combine the prediction and decision-making steps into a single end-to-end differentiable pipeline, e.g., as in Wilder, et al. [3]:\nHowever, this has a couple of problems:\nDifferentiating through optimization problems is non-trivial, especially for optimization tasks in which the decisions are discrete (e.g., in the advertising example where you can only decide to either choose a website or not). This is because, in such problems, the gradients of the optimal decision with respect to the input parameters are not defined, and so creating an end-to-end pipeline requires coming up with a differentiable “surrogate optimization problem”. As a consequence, it can be difficult to use DFL for a new decision-making task. Moreover, even if we could differentiate through the decision-making step, a single gradient update requires solving an optimization problem and subsequently differentiating through it. This can be computationally expensive, especially for complex optimization problems. Instead, in a series of papers [1-2] we propose an alternate strategy for performing DFL that does not require differentiable optimization but instead aims to distill the task-specific information into a “decision loss”. In the remainder of this article, I describe this approach in more detail and hope to convince you of its utility.\nLearning A Decision Loss What is a Decision Loss? One way in which we can interpret the PtO framework from a prediction point of view, is to see the Optimize + Evaluate steps as defining a loss function, i.e., a mapping from predictions and true labels to decision quality. As a result, we can see these two steps as inducing a “decision loss” that provides information about how good a given prediction is in terms of its decision quality.\nHowever, changing the framing does not solve any of the differentiability or efficiency challenges that we’ve described above. The key insight is that, rather than evaluating this decision loss exactly in each forward and backward pass, we instead learn a surrogate for the decision loss that is cheap to evaluate and has nice properties, e.g., convexity and consistency. Broadly, we learn this decision loss by first generating a supervised learning dataset using which we can learn a loss function, and next fitting a loss function with nice properties to this dataset. We describe the approach in more detail below.\nMeta-Algorithm for Learning a Decision Loss Concretely, the three steps involved in learning a loss function are as follows:\n(Step 1) Generate Inputs: The first step in creating a dataset for supervised learning is to generate the inputs. For the decision loss, there are two inputs—the predicted parameters and the true parameters. While the true parameters are assumed to be part of the PtO dataset, the predictions are unknown at the time of generating a loss function. As a result, this first step involves generating realistic samples of what such predictions could be: \\ In [1], we use a simple “localness” heuristic that uses the assumption that your model will do reasonably well at predicting the true parameters and, as a result, you can model your predictions as the true parameters + Gaussian noise.\n(Step 2) Generate Outputs: Given different samples of possible predictions, the next step is to run the Optimize + Evaluate steps of the PtO pipeline in order to generate the decision qualities that correspond to all of these different input predictions. Note that this does not require being able to differentiate through the optimization problem, just the ability to solve it. The combination of these inputs and outputs generates a supervised learning dataset.\n(Step 3) Fit Loss: Lastly, we fit a decision loss with nice properties to this dataset. In [1], we learn a different loss function for every set of true parameters. These loss functions can be thought of as the first and second-order Taylor expansions of (a perturbed version of) the decision loss at a given set of true parameters. Concretely the first-order expansion can be seen as a “Weighted MSE” in which we have a weight associated with each parameter that needs to be estimated; a higher weight means that the decision loss is more sensitive to errors in estimating that parameter. \\\nFinally, we can then train our predictive model using this learned decision loss and the PtO dataset to generate a task-specific predictive model.\nDoes this work? We call our learned loss functions from [1] `Locally Optimized Decision Losses’, or LODLs for short. We test their performance on two tasks from the literature and one synthetic domain designed for analysis and ablation. We compare them to both “naively” learning the prediction model in a task-agnostic fashion and the end-to-end differentiable approach from past work (for which each surrogate is customized for the chosen task). As you can see in the results (from [2]) below, we find that LODLs can significantly outperform not only the naive baseline but even their end-to-end counterparts from the literature, suggesting the promise of this approach.\nImproving the Decision Loss In [2], we note that most of the time spent to learn LODLs is spent in Step 2, in which we have to make numerous calls to an optimization solver to create the outputs for our loss dataset. However, there isn’t much we can do to speed up solvers for arbitrary optimization problems. Instead, in this follow-up paper we try to modify steps 1 and 3 of the meta-algorithm so that we can learn good loss functions with a smaller dataset and, as a result, we have to make fewer calls to the optimization solver in Step 2.\nBetter Predictions (Step 1) To learn the loss functions, the first step of our meta-algorithm involves generating potential predictions. However, this is hard because predictions are typically the output of a prediction model; but to generate such a predictive model, you need a loss function to train it on, resulting in a circular dependency. While [1] uses a “localness”-based heuristic to generate predictions, this does not yield very realistic parameters.\nThis lack of realistic predictions is a problem because the underlying process that we are trying to model is complicated. As a result, it is unlikely that we will be able to accurately estimate the decision loss for arbitrary predictions. Instead, we want to train it on realistic predictions so that when you use the loss function to train a predictive model, the distribution shift between what it is trained on and what it is used for is minimized.\nIn [2], we use the outputs of a model trained on the MSE loss with both different random initializations and at different points in the training trajectory to generate realistic predictions. While the MSE loss and decision loss are different, we believe that because this approach takes into account the structure of both that data and the model, it samples from a more realistic space of possible predictions.\nBetter Losses (Step 3) In Step 3 of the meta-algorithm, the goal is to fit a loss function to the dataset created in steps 1 and 2.\nIn [1], we learn a different “local” loss functions (i.e., a different set of weights) for each set of true parameters. However, because these parameters are learned independently for each set of true parameters, there’s no way information sharing across these different loss functions, making the learning process sample inefficient.\nInstead, in [2], we learn a single neural network that takes as input the features associated with a given parameter (e.g., website and user type data) and uses that to estimate the corresponding weight of the parameter in the WeightedMSE loss function. Because we’re sharing information via this neural network, the resulting learning process ends up being significantly more sample-efficient.\nResults To summarize, we don’t change this overall algorithm for learning the decision loss, but instead try to make learning loss functions more sample efficient. We do this by modifying steps 1 and 3 using what we call model-based sampling and feature-based parameterization, leading to our overall approach which we call “Efficient Global Losses”, or EGLs, because both our improvements stem from moving away from “local” heuristics used by past work. We compare EGLs to LODLs in the table above on the same three PtO tasks as [1].\nIn terms of performance, we find that EGLs perform on par with or better than LODLs with only 32 samples as opposed to 2048. This means that we need to make significantly fewer calls to the optimization solver for the same performance.\nIn terms of computational cost, we see that by reducing the number of samples from 2048 to 32, the time taken during step 2 by EGLs decreases by 50x. Moreover, even though we now have more complex implementations of steps 1 and 3, we still have a 16x improvement in overall computational cost.\nBefore concluding, I just wanted to sneak in that we have additional results in [2] about how improving Step 1 is especially useful for low-capacity models, and how the improved version of Step 3 has good theoretical properties (Fisher Consistency). If you’re interested, please have a look at our paper!\nConclusions We create an alternate approach for learning decision-focused predictive models for PtO by creating task-specific loss functions. This approach doesn’t require differentiating through optimization problems and we show in [1-2] that it even leads to improved empirical performance in practice. We believe that by reducing DFL to the supervised learning problem of learning a loss function, this approach brings task-specific predictive models closer to being usable in practice. We also believe that this is fertile ground for new work, as evidenced by the numerous recent follow-up works [4-6].\nIf you find this interesting (or confusing?) and would like to chat more about it, shoot me an email at sanketshah@g.harvard.edu!\nReferences: [1] Shah, Sanket, et al. \u0026ldquo;Decision-focused learning without decision-making: Learning locally optimized decision losses.\u0026rdquo; Advances in Neural Information Processing Systems 35 (2022): 1320-1332. [2] Shah, Sanket, et al. \u0026ldquo;Leaving the Nest: Going Beyond Local Loss Functions for Predict-Then-Optimize.\u0026rdquo; Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 38. 2024. [3] Wilder, Bryan, Bistra Dilkina, and Milind Tambe. \u0026ldquo;Melding the data-decisions pipeline: Decision-focused learning for combinatorial optimization.\u0026rdquo; Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 33. No. 01. 2019. [4] Zharmagambetov, Arman, et al. \u0026ldquo;Landscape surrogate: Learning decision losses for mathematical optimization under partial information.\u0026rdquo; Advances in Neural Information Processing Systems 36 (2024). [5] Bansal, Dishank, et al. \u0026ldquo;Taskmet: Task-driven metric learning for model learning.\u0026rdquo; Advances in Neural Information Processing Systems 36 (2024). [6] Jeon, Haeun, et al. \u0026ldquo;ICLN: Input Convex Loss Network for Decision Focused Learning.\u0026rdquo; arXiv preprint arXiv:2403.01875 (2024).\n","date":1714521600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1714521600,"objectID":"79547762950a7ce82248a0180d576e4b","permalink":"https://sanketkshah.github.io/post/loss-functions/","publishdate":"2024-05-01T00:00:00Z","relpermalink":"/post/loss-functions/","section":"post","summary":"A blog post about the predict-then-optimize paradigm for algorithmic decision-making, and how we can view it as inducing a task-specific loss function in supervised machine learning.","tags":["Decision-Focused Learning","Machine Learning"],"title":"Learning Loss Functions for Predict-then-Optimize","type":"post"},{"authors":["Sanket Shah","Bryan Wilder","Andrew Perrault","Milind Tambe"],"categories":null,"content":"","date":1708732800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1708732800,"objectID":"3abba7ff0c44163a7f6c4cce465d911e","permalink":"https://sanketkshah.github.io/publication/dfl-egl/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/publication/dfl-egl/","section":"publication","summary":"We propose two new innovations to help improve the learning of task-specific loss functions.","tags":["Decision-Focused Learning","Machine Learning"],"title":"Leaving the Nest: Going Beyond Local Loss Functions for Predict-Then-Optimize","type":"publication"},{"authors":["Sanket Shah","Shresth Verma","Amrita Mahale","Kumar Madhu Sudan","Aparna Hegde","Aparna Taneja","Milind Tambe"],"categories":null,"content":"","date":1683417600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1683417600,"objectID":"d3849ea37236fa688d4346b73c108043","permalink":"https://sanketkshah.github.io/publication/kilkari/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/publication/kilkari/","section":"publication","summary":"We perform preliminary analysis on the Kilkari mobile health program and find that past approaches to predicting listenership in mobile health programs don't work well.","tags":["Public Health","ARMMAN"],"title":"Preliminary Results in Low-Listenership Prediction in One of theLargest Mobile Health Programs in theWorld","type":"publication"},{"authors":["Kai Wang","Shresth Verma","Aditya Mate","Sanket Shah","Aparna Taneja","Neha Madhiwalla","Aparna Hegde","Milind Tambe"],"categories":null,"content":"","date":1675728000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675728000,"objectID":"295ea8340d966801595a0aca382a473c","permalink":"https://sanketkshah.github.io/publication/dfl-armann/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/publication/dfl-armann/","section":"publication","summary":"We propose a way to differentiate through MDP Planning for Restless Multi-Armed Bandits. We use this approach to better learn the Transition Matrices from \"features\" associated with different arms using Decision-Focused Learning.","tags":["Decision-Focused Learning","Reinforcement Learning","Restless Multi-Armed Bandits"],"title":"Decision-Focused Learning in Restless Multi-Armed Bandits with Application to Maternal and Child Care Domain","type":"publication"},{"authors":["Sanket Shah","Kai Wang","Bryan Wilder","Andrew Perrault","Milind Tambe"],"categories":null,"content":"","date":1671062400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1671062400,"objectID":"1cf4ade7bad5ee96ab3afa31884f743a","permalink":"https://sanketkshah.github.io/publication/dfl-lodl/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/publication/dfl-lodl/","section":"publication","summary":"We learn task-specific loss functions that, when trained on, allow a predictive model to make better predictions for the given task.","tags":["Decision-Focused Learning","Machine Learning"],"title":"Decision-Focused Learning without Decision-Making: Learning Locally Optimized Decision Losses","type":"publication"},{"authors":["Sanket Shah","Meghna Lowalekar","Pradeep Varakantham"],"categories":null,"content":"","date":1655078400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655078400,"objectID":"6cb6da4830c8b56037756d4e4458902a","permalink":"https://sanketkshah.github.io/publication/pricing-matching/","publishdate":"2022-06-13T00:00:00Z","relpermalink":"/publication/pricing-matching/","section":"publication","summary":"We learn how to price in ride-pooling (UberPool) while taking into account the matchings the pricing system induces.","tags":["Ride-Pooling","Transportation","Mechanism Design"],"title":"Joint Pricing and Matching for City-Scale Ride-Pooling","type":"publication"},{"authors":["Kai Wang","Sanket Shah","Haipeng Chen","Andrew Perrault","Finale Doshi-Velez","Milind Tambe"],"categories":null,"content":"","date":1638748800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638748800,"objectID":"a0c154815df8fe56a46eb1e1e9375b91","permalink":"https://sanketkshah.github.io/publication/dfl-mdp/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/publication/dfl-mdp/","section":"publication","summary":"We propose a way to optimally differentiate through Reinforcement Learning. Specifically, we propose two optimality conditions that hold at convergence and show how to (approximately) calculate gradients using them.","tags":["Decision-Focused Learning","Reinforcement Learning","Implicit Layers"],"title":"Learning MDPs from Features: Predict-Then-Optimize for Sequential Decision Problems by Reinforcement Learning","type":"publication"},{"authors":["Ashwin Kumar","Sanket Shah","Meghna Lowalekar","Pradeep Varakantham","Alvitta Ottley","William Yeoh"],"categories":null,"content":"","date":1629504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629504000,"objectID":"93f7d09d6c16f5bdea16e27ef47e3ff8","permalink":"https://sanketkshah.github.io/publication/fairvizard/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/publication/fairvizard/","section":"publication","summary":"We create a visualization to allow various stakeholders to evaluate the tradeoffs associated with multi-party fairness in the Ride-Pooling (Uber) ecosystem.","tags":["Ride-Pooling","Transportation","Fairness","Visualization"],"title":"FairVizARD: A Visualization System for Assessing Fairness of Ride-Sharing Matching Algorithms","type":"publication"},{"authors":["Naveen Raman","Sanket Shah","John Dickerson"],"categories":null,"content":"","date":1627776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627776000,"objectID":"abbef50b73dcaba333c5a7f83ad5fbf4","permalink":"https://sanketkshah.github.io/publication/fairness-naveen/","publishdate":"2021-08-01T00:00:00Z","relpermalink":"/publication/fairness-naveen/","section":"publication","summary":"We evaluate the efficacy of two different strategies of enforcing fairness in ride-pooling.","tags":["Ride-Pooling","Transportation","Fairness"],"title":"Data-Driven Methods for Balancing Fairness and Efficiency in Ride-Pooling","type":"publication"},{"authors":["Jackson Killian","Arpita Biswas","Sanket Shah","Milind Tambe"],"categories":null,"content":"","date":1597363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597363200,"objectID":"94393ac0e934a0cd2c3f7a0c78c0069b","permalink":"https://sanketkshah.github.io/publication/multi-action-rmab/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/publication/multi-action-rmab/","section":"publication","summary":"We propose two online model-free algorithms to learn the Whittle Index associated with *multi-action* Restless Multi-Armed Bandits.","tags":["Reinforcement Learning","Restless Multi-Armed Bandits"],"title":"Q-Learning Lagrange Policies for Multi-Action Restless Bandits","type":"publication"},{"authors":["Sanket Shah","Meghna Lowalekar","Pradeep Varakantham"],"categories":null,"content":"","date":1581033600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581033600,"objectID":"fd9f7ebb9c9b8fa0399125c97cb21786","permalink":"https://sanketkshah.github.io/publication/ride-pooling/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/publication/ride-pooling/","section":"publication","summary":"We add future information to ride-pooling assignments by using a novel extension to Approximate Dynamic Programming.","tags":["Ride-Pooling","Transportation","Approximate Dynamic Programming","Deep Reinforcement Learning"],"title":"Neural Approximate Dynamic Programming for On-Demand Ride-Pooling","type":"publication"},{"authors":["Sanket Shah","Arunesh Sinha","Pradeep Varakantham","Andrew Perrault","Milind Tambe"],"categories":null,"content":"","date":1581033600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581033600,"objectID":"b068e258bba8a1ebfd53d4e2b54a8319","permalink":"https://sanketkshah.github.io/publication/tsg/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/publication/tsg/","section":"publication","summary":"We reformulate Threat Screening Games, a kind of Stackelberg Security Game, as an MDP with constraints on the action space.","tags":["Game Theory","Deep Reinforcement Learning","Security"],"title":"Solving Online Threat Screening Games using Constrained Action Space Reinforcement Learning","type":"publication"},{"authors":["Sanket Shah"],"categories":["Blog"],"content":"Before joining SMU, I lived in my hometown of Bangalore in India. It is a city that, much to my chagrin, has become synonymous with its terrible traffic. The absence of comprehensive public transport has meant that, often, the most convenient alternative to owning a vehicle is in using taxi-on-demand services from aggregators like Uber and Ola. The affordability of ride-pooling services (like UberPool) have made them especially integral to the transportation ecosystem with taxi-sharing often being the cheapest means of on-demand transportation. However, while ride-pooling services are popular, they are far from efficient. I would often find myself trapped in circuitous two-hour long UberPool journeys, obsessing over ways to do things better. As a result, when an opportunity to work with my current advisor on such ride-pooling systems presented itself, I jumped at it.\nAt SMU, I started thinking of solutions to this problem in earnest\u0026mdash;how do you match customers to vehicles so that people would no longer have to circumnavigate Bangalore on their way to work. With lots of help from my collaborators, I went through the literature, especially the erstwhile state-of-the-art method for high-capacity city-scale ride-pooling \u0026lsquo;On-demand high-capacity ride-sharing via dynamic trip-vehicle assignment\u0026rsquo;. In this approach, they enumerated the feasible assignments of passengers to a vehicle and then ran an optimisation over these feasible assignments to choose the best combination of assignments over all vehicles. The time-consuming portion of this process was in enumerating these feasible assignments because there are an exponential number of them (in the worst case) and each involves solving an NP-hard routing problem. As a result, in practice, one can only generate a subset of all the feasible assignments.\nAccordingly, given that all feasible assignments can\u0026rsquo;t be enumerated, our first attempt at improving their solution was in trying to generate a \u0026lsquo;better\u0026rsquo; subset of them. We did this by \u0026rsquo;learning to search\u0026rsquo;, which is similar to A* search in which the heuristic is learned. The novel idea here was to make learning this heuristic unsupervised by using the weak supervision from the output of the optimisation problem\u0026mdash;we scored assignments based on how likely they were to be chosen as the optimal assignment. We found, however, that enumerating \u0026lsquo;better\u0026rsquo; (or even more) assignments didn\u0026rsquo;t improve the solution quality much. Instead, from visualising some trajectories, we found that a bigger issue was that the optimisation was greedy, i.e., it tried to maximise the matches at the current time-step without thought for how this would affect future matches.\nTo solve this problem, during the optimisation step, we tried to score the enumerated feasible assignments based on their expected future value rather than their immediate value. This idea has connections to learning a value function in the Q-Learning approach to RL, in which the optimal policy involves choosing the \u0026lsquo;action\u0026rsquo; (overall assignment) with the maximum future value. The key insight was that, if we could find a way to estimate this future value for a given feasible assignment, we could use the existing optimisation over feasible assignments to approximate this \u0026lsquo;max\u0026rsquo; operation in Q-Learning. Along these lines, I developed an algorithm for estimating this future value of a feasible assignment and tested it on real-world data \u0026ndash; it worked! We found that it outperformed the greedy method by up to 16% (which is huge on a city-scale).\nThis is where I thought the project would end. However, we hit a surprising roadblock \u0026ndash; we learned that, although the algorithm was \u0026lsquo;inspired\u0026rsquo; by Q-Learning, the modifications we made to it meant that it was no longer straightforward to use the theoretical results associated with Q-Learning to illustrate the soundness of our algorithm. The final part of this project involved finding a more \u0026rsquo;natural\u0026rsquo; way to communicate what the algorithm was doing and why it worked. To this end, we did some reading and found that we had re-invented Approximate Dynamic Programming (ADP), a framework from Operations Research that could be seen as a way to combine traditional optimisation with learned value functions to perform stochastic planning. The discovery allowed us to use the language and theory that they had (much more rigorously) developed to better analyse and discuss our algorithm.\nIf you are interested in learning more about the technical details of our algorithm, please check out the resulting paper here. The paper was recently accepted for Oral presentation at AAAI-20\u0026rsquo;s special track on AI for Social Impact. If you are planning to attend the conference, please drop by for our presentation! We are also currently looking for partners with whom to run a pilot study. If that\u0026rsquo;s you, please reach out.\n","date":1576368000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576368000,"objectID":"09f992d7e915d2882a74e1c8699960d4","permalink":"https://sanketkshah.github.io/post/ride-pooling/","publishdate":"2019-12-15T00:00:00Z","relpermalink":"/post/ride-pooling/","section":"post","summary":"A blog post about the joyride my paper 'Neural Approximate Dynamic Programming for On-Demand Ride-Pooling' took me on.","tags":["Ride-Pooling","Transportation","Approximate Dynamic Programming","Deep Reinforcement Learning"],"title":"My Ride-pooling Journey","type":"post"}]